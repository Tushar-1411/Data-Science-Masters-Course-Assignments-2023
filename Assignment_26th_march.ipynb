{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16f3605e-7f68-4448-a1aa-cdff80f857fc",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "1. Simple Linear Regression : When we try to predict a dependent variable from one independent variable (one input, one output).\n",
    "\n",
    "Example : Predicting price of a house based on the size of the house.\n",
    "\n",
    "2. Multiple Linear Regression : When we have multiple independent variables and one dependent variable.\n",
    "\n",
    "example : Predicting house price from number of rooms, size of house, region etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9f374f-3de0-4d78-bcdb-b7c988bc5cab",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Assumptions of Linear regression : \n",
    "\n",
    "1. Independence : All the features are independent of each other.\n",
    "2. Linear Relationship : All the independent features are linearly dependent to the target variable.\n",
    "3. Normality : The residuals of the model are normally distributed.\n",
    "\n",
    "To check for these assumptions : \n",
    "1. To check if the linear relationship assumption is met, we can create a scatter plot of x vs. y. This allows us to visually see if there is a linear relationship between the two variables.\n",
    "2. To check if the independence assumption is met, we can look at a residual time series plot, which is a plot of residuals vs. time.\n",
    "3. To check if the normality assumption is met, we can use Q-Q plots or formal statistical tests like Shapiro-Wilk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff86b5b-9a52-45c9-bd3f-e4b4f51167be",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "In a linear regression model, the slope and intercept are used to define the linear relationship between two variables. The slope represents the average change in the response variable for a one unit increase in the predictor variable, while the intercept represents the mean value of the response variable when the predictor variable is equal to zero.\n",
    "\n",
    "**Example** : Suppose we want to understand the relationship between the number of hours studied and exam scores for students in a college course. We collect data on the number of hours studied and exam scores for 50 students and fit a simple linear regression model. The resulting equation is: Exam score = 65.4 + 2.67 (hours).\n",
    "\n",
    "In this example, the slope is 2.67, which means that for every additional hour studied, we would expect, on average, an increase of 2.67 points in the exam score. The intercept is 65.4, which represents the average exam score when the number of hours studied is equal to zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e33e9f8-1876-416e-8a45-7dd711b1ab7e",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Gradient Descent is an optimizing technique which is used to find the best fit lines for a regression problem by evaluating the cost function and minimizing it to find the optimal values of the coefficients of slope and intercept and to attain global minima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b550afc-94a0-4576-af76-6202d4d084ef",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Multiple linear regression is a statistical method used to model the relationship between a dependent variable and two or more independent variables. It is an extension of simple linear regression, which models the relationship between a dependent variable and a single independent variable.\n",
    "\n",
    "The main difference between multiple linear regression and simple linear regression is that multiple linear regression can include more than one independent variable. This allows us to model more complex relationships between the dependent and independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8e2a07-8960-4551-bc18-c43ca752cb25",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "Multicollinearity is a common issue that arises in multiple linear regression when two or more independent variables are highly correlated with each other. This means that one independent variable can be linearly predicted from the others with a high degree of accuracy.\n",
    "\n",
    "There are several ways to detect multicollinearity in a multiple linear regression model. One common method is to calculate the variance inflation factor (VIF) for each independent variable. The VIF measures how much the variance of the estimated regression coefficients is inflated due to multicollinearity. A VIF value greater than 5 or 10 is usually considered an indication of high multicollinearity.\n",
    "\n",
    "Another way to detect multicollinearity is to examine the correlation matrix of the independent variables. If two or more independent variables have a high correlation coefficient (e.g., greater than 0.8), this may indicate the presence of multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93fd485-17d4-4582-890c-fc6230ecdac9",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "Polynomial regression is a type of regression analysis in which the relationship between the independent variable x and the dependent variable y is modeled as an nth degree polynomial.\n",
    "\n",
    "The main difference between polynomial regression and linear regression is that polynomial regression can model non-linear relationships between the independent and dependent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d3b28c-643b-48ed-b51f-34ea7c4130d9",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "\n",
    "Advantages of polynomial regression:\n",
    "\n",
    "1. Polynomial regression can model more complex relationships between the independent and dependent variables than linear regression. This can be useful when the relationship between the two variables is non-linear.\n",
    "2. Polynomial regression can provide a better fit to the data than linear regression, resulting in more accurate predictions.\n",
    "\n",
    "Disadvantages of polynomial regression:\n",
    "\n",
    "1. Polynomial regression can be more computationally intensive than linear regression, especially when the degree of the polynomial is high.\n",
    "2. Polynomial regression can be more prone to overfitting than linear regression.Overfitting occurs when the model is too complex and fits the data too well, including the noise in the data. This can result in poor generalization performance when making predictions on new data.\n",
    "\n",
    "In general, polynomial regression is preferred over linear regression when there is evidence of a non-linear relationship between the independent and dependent variables. This can be determined by visual inspection of a scatter plot of the data or by using statistical tests to assess the linearity of the relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cdb4c2-6665-40f9-9128-7e187a840410",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
