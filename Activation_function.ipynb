{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91ff1000-1c6a-4dcf-a2fc-85d7119a1bf4",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "Activation function are functions which are used to introduce non-linearity in the neural network for learning complex patterns.\n",
    "\n",
    "- These functions also filter out the inputs coming in the neurons and they decide whether to activate the neuron or not and further process the output or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64f8d71-e59f-4ee2-865f-9dcbd7d2fdc4",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Some of the activation functions used in neural networks are :\n",
    "\n",
    "1. Sigmoid Function\n",
    "2. Relu\n",
    "3. Leaky relu\n",
    "4. TanH\n",
    "5. Softmax\n",
    "6. Parametric Relu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35214e6b-e4d9-4756-8d9b-af86282862d3",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Activation functions play a crucial role in a neural network's training process and overall performance. They introduce non-linearities to the network, enabling it to learn complex patterns and relationships in the data. The choice of activation function impacts convergence speed, model expressiveness, and the ability to handle different types of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3b2b8e-2051-44c8-a4e0-f00450098d0f",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "The sigmoid function is also called a squashing function as its domain is the set of all real numbers, and its range is (0, 1). Hence, if the input to the function is either a very large negative number or a very large positive number,  the output is always between 0 and 1.\n",
    "\n",
    "Advantages:\n",
    "\n",
    "- Smooth Gradient: The sigmoid function produces a smooth gradient, making it useful in scenarios where a smooth transition between two classes is desired.\n",
    "- Output Range: It squashes the output between 0 and 1, which is interpreted as a probability. This is useful in binary classification problems.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "- Vanishing Gradient: Sigmoid saturates for extreme values of input, causing gradients to be close to zero. This can lead to the vanishing gradient problem, slowing down or halting the learning process in deep networks.\n",
    "- Output not Centered: The output of the sigmoid is not centered around zero, making it less ideal for weight updates in optimization algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165a2f56-73e9-4d30-8c40-3bd66628aac3",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "ReLU Activation Function:\n",
    "\n",
    "f(x)=max(0,x)\n",
    "\n",
    "Differences from Sigmoid:\n",
    "\n",
    "- Non-linearity: ReLU introduces non-linearity, allowing the network to learn complex patterns.\n",
    "- No Saturation: Unlike the sigmoid, ReLU does not saturate for positive inputs, avoiding the vanishing gradient problem.\n",
    "- Sparse Activation: ReLU neurons can be more efficient as they only activate for positive inputs, leading to sparsity in activation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42af72a-661f-4a16-86f8-851ead2a732b",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "Benefits of ReLU over Sigmoid:\n",
    "\n",
    "- No Vanishing Gradient: ReLU helps mitigate the vanishing gradient problem, making it suitable for deep networks.\n",
    "- Computational Efficiency: ReLU is computationally more efficient than the sigmoid, as it involves simple thresholding.\n",
    "- Sparse Activation: ReLU can lead to sparse activation, enabling the model to focus on relevant features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a351fd-7536-4660-8580-484792492201",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "Leaky ReLU:\n",
    "\n",
    "**f(x)=max(αx,x)**, where \n",
    "\n",
    "α is a small positive constant.\n",
    "\n",
    "Leaky ReLU introduces a small slope (α) for negative inputs, preventing complete saturation and addressing the vanishing gradient problem. This ensures that even neurons with negative input values contribute to the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa5bf8f-b573-4279-9f07-95726c347153",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "Softmax Activation Function:\n",
    "\n",
    "Purpose:\n",
    "\n",
    "Softmax is used in the output layer of a neural network for multi-class classification problems. It converts the network's raw output into a probability distribution over multiple classes, making it suitable for selecting the most probable class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f16c151-596e-470b-91af-7d2f5e86c473",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "\n",
    "**Tanh Activation Function:**\n",
    "\n",
    "Comparison with Sigmoid:\n",
    "\n",
    "- Output Range: Tanh squashes the output between -1 and 1, making it zero-centered. This can help mitigate issues related to optimization in certain scenarios.\n",
    "- Similarities: Like the sigmoid, tanh suffers from the vanishing gradient problem for extreme values of input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf4184f-9dcc-46f0-ada3-9754c2d0eb28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
