{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b14ac1d9-0c5a-460f-a993-83bf3ea116d4",
   "metadata": {},
   "source": [
    "**Q1. What is the KNN algorithm?**\n",
    "KNN, or k-Nearest Neighbors, is a supervised machine learning algorithm used for classification and regression tasks. It classifies a data point based on the majority class of its k-nearest neighbors in the feature space.\n",
    "\n",
    "**Q2. How do you choose the value of K in KNN?**\n",
    "Choosing the right value of K is crucial. Small values of K can make the model sensitive to noise, while large values can smooth out patterns. Cross-validation or grid search is often used to find an optimal K value.\n",
    "\n",
    "**Q3. What is the difference between KNN classifier and KNN regressor?**\n",
    "KNN classifier predicts the class of a data point, while KNN regressor predicts a continuous value. In classification, the output is a class label, whereas in regression, it's a numeric value.\n",
    "\n",
    "**Q4. How do you measure the performance of KNN?**\n",
    "Common metrics include accuracy for classification and mean squared error for regression. Precision, recall, and F1-score are also used for classification tasks.\n",
    "\n",
    "**Q5. What is the curse of dimensionality in KNN?**\n",
    "As the number of dimensions/features increases, the distance between data points becomes less meaningful, leading to sparse data. This can affect the performance of KNN, and dimensionality reduction techniques may be needed.\n",
    "\n",
    "**Q6. How do you handle missing values in KNN?**\n",
    "Imputation methods, such as replacing missing values with the mean or median of the feature, can be used. Alternatively, the algorithm can be adapted to handle missing values during distance computation.\n",
    "\n",
    "In KNN, each data point is classified or predicted based on the characteristics of its neighbors. The choice of K, handling missing values, and understanding the impact of dimensionality are critical aspects of effectively using the KNN algorithm.\n",
    "\n",
    "**Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?**\n",
    "\n",
    "- *KNN Classifier:* It's effective for classification tasks where the output is a categorical variable. It performs well when decision boundaries are not linear. However, it might struggle when there are many irrelevant features or when classes are not well-separated.\n",
    "\n",
    "- *KNN Regressor:* Suitable for regression problems where the output is a continuous variable. It works well when relationships between features and target variables are not linear. It can be sensitive to outliers, and the choice of K is crucial for its performance.\n",
    "\n",
    "The choice between classifier and regressor depends on the nature of the problem and the type of output variable.\n",
    "\n",
    "**Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?**\n",
    "\n",
    "- *Strengths:* Simple to implement, no training phase, and can adapt to complex decision boundaries.\n",
    "  \n",
    "- *Weaknesses:* Sensitive to outliers and irrelevant features, computationally expensive for large datasets, and needs careful selection of K.\n",
    "\n",
    "To address weaknesses, preprocessing techniques like feature scaling, outlier handling, and dimensionality reduction can be applied. Cross-validation helps in optimal K selection.\n",
    "\n",
    "**Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?**\n",
    "\n",
    "- *Euclidean Distance:* Straight-line distance between two points. It is suitable for continuous and normally distributed data.\n",
    "\n",
    "- *Manhattan Distance:* Sum of absolute differences between the coordinates. It is robust to outliers and works well with data that doesn't follow a normal distribution.\n",
    "\n",
    "The choice between them depends on the nature of the data and the problem.\n",
    "\n",
    "**Q10. What is the role of feature scaling in KNN?**\n",
    "\n",
    "Feature scaling ensures that all features contribute equally to the distance computation. Since KNN relies on distance measures, features with larger scales can dominate the distance calculation. Common techniques include min-max scaling or standardization. Feature scaling helps KNN perform better and be more robust to differences in feature magnitudes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
