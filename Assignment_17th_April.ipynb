{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbd4f9d1-1417-4e4c-be3e-f936b596d606",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "Gradient Boosting Regression is a machine learning technique used for regression problems. It is an ensemble learning method that combines multiple weak learners (typically decision trees) to create a strong predictive model. It works by iteratively fitting new models to the errors (residuals) made by the previous models, thereby reducing the overall error and improving predictive accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ea63dd-f986-42b6-8953-31bf0298fce8",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "In the context of gradient boosting, a weak learner is a simple model or predictor that performs slightly better than random guessing. Weak learners are typically shallow decision trees with limited depth or other simple models. They are called \"weak\" because they have limited predictive power on their own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a2afa9-bb69-45a6-8579-eebee2f0622a",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "The intuition behind Gradient Boosting is to combine multiple weak learners to create a strong predictive model. It does this by iteratively fitting new models to the errors (residuals) made by the previous models, effectively \"boosting\" the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81bba40-bdae-4c8f-843d-615469cb55eb",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "Gradient Boosting builds an ensemble by training a series of weak learners sequentially. At each step, a new weak learner is trained to predict the residuals (errors) made by the previous ensemble. These residuals are adjusted based on the learning rate, and the process continues until a predefined number of iterations is reached. The final prediction is the sum of all the learners' predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8517d0-c724-49c2-b243-1b6f75172850",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "- Initialize the model with a simple prediction, often the mean of the target values.\n",
    "- Calculate the residuals (errors) between the actual target values and the initial prediction.\n",
    "- Train a weak learner (e.g., a shallow decision tree) to predict these residuals.\n",
    "- Adjust the residuals based on a learning rate and add the weak learner's predictions to the initial prediction.\n",
    "- Repeat steps 2-4 for a specified number of iterations, with each new weak learner predicting the residuals of the current ensemble.\n",
    "- The final prediction is the sum of all the learners' predictions, which combines their contributions based on the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9633ed5f-5d6f-45a9-8c9d-8cd1f945fa23",
   "metadata": {},
   "source": [
    "## Question 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71806934-49f0-41ea-8be4-be1e04838f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 2898.4366729135227\n",
      "R-squared: 0.4529343796683364\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the sample dataset \n",
    "diabetes = load_diabetes()\n",
    "data = pd.DataFrame(data=diabetes.data, columns=diabetes.feature_names)\n",
    "target = diabetes.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Instantiate the GradientBoostingRegressor\n",
    "gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "gbr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = gbr.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ed616e2-3ee6-41da-89e2-4f042e7be1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'n_estimators': 50, 'max_depth': 3, 'learning_rate': 0.05}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define a grid of hyperparameters to search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = RandomizedSearchCV(GradientBoostingRegressor(), param_grid, cv=5)\n",
    "\n",
    "# Perform the grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa5724c-b3de-421f-bbb4-e5d1fd8d9b10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
