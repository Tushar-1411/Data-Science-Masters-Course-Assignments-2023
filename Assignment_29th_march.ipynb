{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a58745c-4ce1-40e9-97d1-a684d1236a3c",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "Lasso Regression is a special type of regression method which uses the concept of regularization {L1 penalty term} and adds a penalty term to the OlS regression equation to avoid overfitting and it is also used to make feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5b208b-47b5-4dea-9d11-b8cbdbd8fbd3",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "The main advantage of using lasso regression in feature selection is that it makes the coefficients of the features which arent much useful in determining the target variable as zero hence removing them from the equation ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cb6c80-be85-4487-9698-2dfe6182b804",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "The coefficients of a Lasso Regression model are interpreted in the same way as other linear regression models, but it is important to keep in mind that Lasso Regression performs L1 regularization, which can result in some coefficients being set to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d294a1d-6a90-494d-afe8-fbd4a9b3f00b",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "In Lasso Regression, the primary tuning parameter is the regularization parameter (often denoted as Î» or alpha), which controls the strength of the regularization applied to the model. The regularization parameter affects the model's performance by influencing the trade-off between fitting the data closely and preventing overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4965da0b-08d3-4b5b-8cc1-b87a567abc2d",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Lasso Regression is primarily designed for linear regression problems, meaning it aims to model linear relationships between predictor variables and the response variable. However, Lasso can be extended to handle non-linear regression problems by incorporating transformations of the original predictors.\n",
    "\n",
    "For example, you can transform the original features using non-linear functions such as polynomial transformations, exponential transformations, logarithmic transformations, etc. Once you've transformed the features, you can apply Lasso Regression to the transformed data. This approach allows Lasso to capture non-linear relationships between predictors and the response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfd784f-b25f-4235-ad03-612879a3f154",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "Ridge Regression and Lasso Regression are both regularization techniques used to address multicollinearity and prevent overfitting in linear regression models. The main differences between the two are in the type of penalty they apply and the impact on the coefficients:\n",
    "\n",
    "1. Penalty Type: Ridge Regression adds the sum of squared coefficients (L2 norm) as a penalty term to the objective function. Lasso Regression adds the sum of absolute coefficients (L1 norm) as a penalty term.\n",
    "\n",
    "2. Coefficient Shrinkage: Ridge Regression shrinks coefficients towards zero without setting them exactly to zero. Lasso Regression has the ability to drive some coefficients exactly to zero, effectively performing feature selection.\n",
    "\n",
    "3. Variable Selection: Due to its L1 penalty, Lasso Regression can lead to sparse models where some coefficients are exactly zero. This makes Lasso Regression suitable for feature selection.\n",
    "\n",
    "4. Multicollinearity Handling: Both Ridge and Lasso Regression handle multicollinearity, but Lasso's ability to set coefficients to zero makes it more aggressive in feature selection in the presence of multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf04706-0ec1-4f36-93f6-351e21cb0b39",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "Yes, Lasso Regression can handle multicollinearity among input features to some extent. When there is multicollinearity, Lasso Regression will tend to shrink coefficients towards zero, and some of them may be driven exactly to zero. This means that Lasso can effectively select a subset of features, favoring the most important ones and ignoring redundant or less influential ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8bb813-43ad-40b4-b166-f90f728cec99",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "The choice of the regularization parameter (lambda) in Lasso Regression is critical for the model's performance. One common approach to select the optimal value of lambda is cross-validation. Here's how you can do it:\n",
    "\n",
    "1. Grid Search: Define a range of lambda values to explore.\n",
    "\n",
    "2. Cross-Validation: Split your dataset into training and validation sets. For each lambda value, fit the Lasso Regression model on the training set and evaluate its performance on the validation set.\n",
    "\n",
    "3. Choose Optimal Lambda: Select the lambda that results in the best performance (e.g., lowest mean squared error) on the validation set.\n",
    "\n",
    "4. Final Model: Train the Lasso Regression model on the entire dataset using the chosen lambda."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
