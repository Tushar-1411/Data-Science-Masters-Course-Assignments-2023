{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d705ea5-560e-42e0-8003-49cb640f810f",
   "metadata": {},
   "source": [
    "### question 1\n",
    "\n",
    "Linear Regression:\n",
    "\n",
    "- Used for predicting continuous numerical outcomes.\n",
    "- The response variable is quantitative.\n",
    "- The goal is to find the best-fitting line (or hyperplane in higher dimensions) that minimizes the sum of squared errors.\n",
    "\n",
    "Example: Predicting house prices based on features like area, number of bedrooms, and location.\n",
    "Logistic Regression:\n",
    "\n",
    "Used for predicting binary outcomes or probabilities.\n",
    "- The response variable is categorical (binary).\n",
    "- The goal is to model the probability that an instance belongs to a particular class using the logistic function.\n",
    "\n",
    "Example: Predicting whether an email is spam or not based on features like word frequency and sender.\n",
    "\n",
    "\n",
    "Scenario where Logistic Regression is More Appropriate:\n",
    "Consider a scenario where you are developing a model to predict whether a customer will churn (cancel their subscription) or not. The outcome is binary (churn or no churn), making it a suitable scenario for logistic regression. The goal is to estimate the probability of churn based on various customer characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1da0c2-42fb-4840-b565-597aca28f57b",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "The cost function in logistic regression is the logistic loss, also known as the cross-entropy loss or log loss:\n",
    "\n",
    "- Cost(hθ(x), y) = −log(hθ(x)) , if y = 1 \n",
    "- Cost(hθ(x), y) = −log(1−hθ(x)) , if y = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5411fa-11d3-4d7c-9944-ddd3c48c2d99",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Regularization is a technique used to prevent overfitting by adding a penalty term to the cost function. In logistic regression, two common types of regularization are L1 (Lasso) and L2 (Ridge) regularization.\n",
    "\n",
    "- L1 Regularization (Lasso): Adds the absolute value of the coefficients as a penalty term. It can lead to feature selection by driving some coefficients to exactly zero.\n",
    "\n",
    "- L2 Regularization (Ridge): Adds the sum of squared coefficients as a penalty term. It encourages small coefficients for all features.\n",
    "\n",
    "Regularization helps prevent overfitting by discouraging large coefficient values that might lead to complex and unstable models. The strength of regularization is controlled by a hyperparameter (lambda) that needs to be tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d8328e-8356-4f78-824e-d4fc48714ca3",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "The ROC curve helps visualize the trade-off between sensitivity and specificity. A perfect model's ROC curve would reach the top-left corner (100% sensitivity and 100% specificity), while a random model's curve would be close to a diagonal line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a202dcdf-fe9d-4ab5-b38f-4c06ae7e94d1",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Feature selection aims to choose the most relevant features for the model, leading to improved model performance, reduced complexity, and faster training times. Some common techniques are:\n",
    "\n",
    "Recursive Feature Elimination (RFE): Iteratively removes the least important features based on model performance.\n",
    "\n",
    "- L1 Regularization (Lasso): L1 regularization inherently performs feature selection by driving some coefficients to zero.\n",
    "\n",
    "- Tree-Based Methods: Decision trees and random forests can be used to measure feature importance and select the most informative features.\n",
    "\n",
    "- Mutual Information: Measures the dependency between features and the target variable.\n",
    "\n",
    "- Feature Importance from Models: Some models (like ensemble methods) provide feature importance scores that can guide selection.\n",
    "\n",
    "- Principal Component Analysis (PCA): Transforms the original features into a new set of uncorrelated features.\n",
    "\n",
    "- SelectKBest: Selects the top k features based on statistical tests like chi-square or ANOVA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8f4f73-e0ed-44db-bb2b-d90bbd704635",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "Imbalanced datasets, where one class has significantly fewer samples than the other, can lead to biased model performance. Strategies to handle class imbalance include:\n",
    "\n",
    "1. Resampling: Oversampling the minority class or undersampling the majority class to balance class distribution.\n",
    "\n",
    "2. Synthetic Data Generation: Techniques like SMOTE (Synthetic Minority Over-sampling Technique) create synthetic samples of the minority class.\n",
    "\n",
    "3. Cost-Sensitive Learning: Assign higher misclassification costs to the minority class to balance the model's focus.\n",
    "\n",
    "4. Ensemble Methods: Algorithms like Random Forest and Gradient Boosting can handle imbalanced data well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1361f892-6452-4ecd-af7c-1281c8705ee9",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "- Multicollinearity: When independent variables are correlated, it can impact coefficient interpretation. Address it using techniques like PCA, dropping correlated features, or L2 regularization.\n",
    "\n",
    "- Overfitting: Logistic regression can overfit complex datasets. Regularization techniques like L1 and L2 can mitigate this.\n",
    "\n",
    "- Convergence Issues: Logistic regression might not converge if learning rates are too high. Adjust learning rates or use techniques like gradient clipping.\n",
    "\n",
    "- Outliers: Outliers can affect model performance. Detect and handle outliers through techniques like robust regression or outlier removal.\n",
    "\n",
    "- Non-Linearity: Logistic regression assumes linear relationship. For non-linear patterns, feature engineering or using non-linear models might be necessary.\n",
    "\n",
    "- Imbalanced Data: Address class imbalance using the strategies mentioned earlier.\n",
    "\n",
    "- Feature Selection: Choosing the right features is crucial. Use techniques like RFE or domain knowledge to avoid irrelevant features.\n",
    "\n",
    "- Interpretability: Logistic regression's coefficients are interpretable, but complex interactions might be missed. Consider using more complex models when interactions are important.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1216a5c-4ff7-468b-8856-294388355756",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
